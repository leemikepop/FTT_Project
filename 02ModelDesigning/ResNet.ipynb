{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob,pandas,time\n",
    "\n",
    "import torch,torch.nn,torchvision\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean&STD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fp = \"../01DataPreProcessing/crop_img/\"\n",
    "RGB_mean = torch.tensor([0,0,0],dtype=torch.float32)\n",
    "RGB_std = torch.tensor([0,0,0],dtype=torch.float32)\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "n = 0\n",
    "for fp in glob.glob(\"../01DataPreProcessing/crop_img/*.jpg\"):\n",
    "    with Image.open(fp) as img:\n",
    "        img = transform(img)\n",
    "        RGB_mean += img.mean(dim=(1,2))\n",
    "        RGB_std += img.std(dim=(1,2))\n",
    "        n+=1\n",
    "RGB_mean /= n\n",
    "RGB_std /= n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self,file_paths,labels_lists,transform):\n",
    "        super().__init__()\n",
    "        self.file_paths = file_paths\n",
    "        self.labels_lists = labels_lists\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        image = Image.open(self.file_paths[index])\n",
    "        image = self.transform(image).float()\n",
    "        labels = self.labels_lists[index]\n",
    "        labels = np.array(labels)\n",
    "        labels = labels.astype('float').reshape(-1,4)\n",
    "        sample = {'image':image,'psi':labels}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_fp,labels_fp = \"../01DataPreProcessing/crop_img/*.jpg\",\"../01DataPreProcessing/ftt_psi.csv\"\n",
    "# df = pandas.read_csv(labels_fp)\n",
    "# df = df.iloc[:,20:24]\n",
    "# psi = df.to_numpy()\n",
    "# img_pl = glob.glob(images_fp)\n",
    "# img_pl = np.array(img_pl)\n",
    "\n",
    "# #shuffle\n",
    "# orig_dataset = np.c_[img_pl,psi]\n",
    "# np.random.seed(1234)\n",
    "# np.random.shuffle(orig_dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BuildDataLoader(images_fp,labels_fp,input_size):\n",
    "    df = pandas.read_csv(labels_fp)\n",
    "    df = df.iloc[:,20:24]\n",
    "    psi = df.to_numpy()\n",
    "    img_pl = glob.glob(images_fp)\n",
    "    img_pl = np.array(img_pl)\n",
    "\n",
    "    #shuffle\n",
    "    orig_dataset = np.c_[img_pl,psi]\n",
    "    np.random.seed(1234)\n",
    "    np.random.shuffle(orig_dataset)\n",
    "\n",
    "    #shuffle\n",
    "    orig_dataset = np.c_[img_pl,psi]\n",
    "    np.random.seed(1234)\n",
    "    np.random.shuffle(orig_dataset)\n",
    "\n",
    "    #split into train and test\n",
    "    train_inputs, test_inputs = [],[]\n",
    "    train_labels, test_labels = [],[]\n",
    "    train_num = int(len(orig_dataset)*0.8)\n",
    "    for data in orig_dataset[:train_num]:\n",
    "        train_inputs.append(data[0])\n",
    "        train_labels.append([data[1:]])\n",
    "    for data in orig_dataset[train_num:]:\n",
    "        test_inputs.append(data[0])\n",
    "        test_labels.append([data[1:]])\n",
    "\n",
    "    # normalize = torchvision.transforms.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
    "    normalize = torchvision.transforms.Normalize(mean = RGB_mean.tolist(), std = RGB_std.tolist())\n",
    "    data_transforms = {\n",
    "        'train': torchvision.transforms.Compose([\n",
    "            torchvision.transforms.RandomResizedCrop(input_size),\n",
    "            torchvision.transforms.RandomHorizontalFlip(),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            normalize\n",
    "        ]),\n",
    "        'val': torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(input_size),\n",
    "            torchvision.transforms.CenterCrop(input_size),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            normalize\n",
    "        ]),\n",
    "        'test': torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize(input_size),\n",
    "            torchvision.transforms.CenterCrop(input_size),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            normalize\n",
    "        ])\n",
    "    }\n",
    "    # train_transformer = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),normalize])\n",
    "    # test_transformer = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),normalize])\n",
    "\n",
    "    train_dataLoader = DataLoader(MyDataSet(train_inputs,train_labels,data_transforms['train']),batch_size=3,shuffle=True)\n",
    "    test_dataLoader = DataLoader(MyDataSet(test_inputs,test_labels,data_transforms['test']),batch_size=3,shuffle=False)\n",
    "    return train_dataLoader,test_dataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataLoader,test_dataLoader = BuildDataLoader(\"../01DataPreProcessing/crop_img/*.jpg\",\"../01DataPreProcessing/ftt_psi.csv\",244)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet50 = torchvision.models.resnet50(weights=torchvision.models.ResNet50_Weights.DEFAULT)\n",
    "print(model_resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (1): Sigmoid()\n",
      "    (2): Linear(in_features=128, out_features=4, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "pre_model_Resnet = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "# print(pre_model_Resnet)\n",
    "#凍結參數\n",
    "for param in pre_model_Resnet.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# pre_model_Resnet.fc = torch.nn.Linear(in_features=512,out_features=4)\n",
    "pre_model_Resnet.fc = torch.nn.Sequential(torch.nn.Linear(in_features=512,out_features=128),torch.nn.Sigmoid(),torch.nn.Linear(in_features=128,out_features=4))\n",
    "pre_model_Resnet = pre_model_Resnet.to(device=device)\n",
    "print(pre_model_Resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t fc.0.weight\n",
      "\t fc.0.bias\n",
      "\t fc.2.weight\n",
      "\t fc.2.bias\n"
     ]
    }
   ],
   "source": [
    "params_to_update = []\n",
    "for name,param in pre_model_Resnet.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\",name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 122, 122]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 122, 122]             128\n",
      "              ReLU-3         [-1, 64, 122, 122]               0\n",
      "         MaxPool2d-4           [-1, 64, 61, 61]               0\n",
      "            Conv2d-5           [-1, 64, 61, 61]          36,864\n",
      "       BatchNorm2d-6           [-1, 64, 61, 61]             128\n",
      "              ReLU-7           [-1, 64, 61, 61]               0\n",
      "            Conv2d-8           [-1, 64, 61, 61]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 61, 61]             128\n",
      "             ReLU-10           [-1, 64, 61, 61]               0\n",
      "       BasicBlock-11           [-1, 64, 61, 61]               0\n",
      "           Conv2d-12           [-1, 64, 61, 61]          36,864\n",
      "      BatchNorm2d-13           [-1, 64, 61, 61]             128\n",
      "             ReLU-14           [-1, 64, 61, 61]               0\n",
      "           Conv2d-15           [-1, 64, 61, 61]          36,864\n",
      "      BatchNorm2d-16           [-1, 64, 61, 61]             128\n",
      "             ReLU-17           [-1, 64, 61, 61]               0\n",
      "       BasicBlock-18           [-1, 64, 61, 61]               0\n",
      "           Conv2d-19          [-1, 128, 31, 31]          73,728\n",
      "      BatchNorm2d-20          [-1, 128, 31, 31]             256\n",
      "             ReLU-21          [-1, 128, 31, 31]               0\n",
      "           Conv2d-22          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-23          [-1, 128, 31, 31]             256\n",
      "           Conv2d-24          [-1, 128, 31, 31]           8,192\n",
      "      BatchNorm2d-25          [-1, 128, 31, 31]             256\n",
      "             ReLU-26          [-1, 128, 31, 31]               0\n",
      "       BasicBlock-27          [-1, 128, 31, 31]               0\n",
      "           Conv2d-28          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-29          [-1, 128, 31, 31]             256\n",
      "             ReLU-30          [-1, 128, 31, 31]               0\n",
      "           Conv2d-31          [-1, 128, 31, 31]         147,456\n",
      "      BatchNorm2d-32          [-1, 128, 31, 31]             256\n",
      "             ReLU-33          [-1, 128, 31, 31]               0\n",
      "       BasicBlock-34          [-1, 128, 31, 31]               0\n",
      "           Conv2d-35          [-1, 256, 16, 16]         294,912\n",
      "      BatchNorm2d-36          [-1, 256, 16, 16]             512\n",
      "             ReLU-37          [-1, 256, 16, 16]               0\n",
      "           Conv2d-38          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-39          [-1, 256, 16, 16]             512\n",
      "           Conv2d-40          [-1, 256, 16, 16]          32,768\n",
      "      BatchNorm2d-41          [-1, 256, 16, 16]             512\n",
      "             ReLU-42          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-43          [-1, 256, 16, 16]               0\n",
      "           Conv2d-44          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-45          [-1, 256, 16, 16]             512\n",
      "             ReLU-46          [-1, 256, 16, 16]               0\n",
      "           Conv2d-47          [-1, 256, 16, 16]         589,824\n",
      "      BatchNorm2d-48          [-1, 256, 16, 16]             512\n",
      "             ReLU-49          [-1, 256, 16, 16]               0\n",
      "       BasicBlock-50          [-1, 256, 16, 16]               0\n",
      "           Conv2d-51            [-1, 512, 8, 8]       1,179,648\n",
      "      BatchNorm2d-52            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-53            [-1, 512, 8, 8]               0\n",
      "           Conv2d-54            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-55            [-1, 512, 8, 8]           1,024\n",
      "           Conv2d-56            [-1, 512, 8, 8]         131,072\n",
      "      BatchNorm2d-57            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-58            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-59            [-1, 512, 8, 8]               0\n",
      "           Conv2d-60            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-61            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-62            [-1, 512, 8, 8]               0\n",
      "           Conv2d-63            [-1, 512, 8, 8]       2,359,296\n",
      "      BatchNorm2d-64            [-1, 512, 8, 8]           1,024\n",
      "             ReLU-65            [-1, 512, 8, 8]               0\n",
      "       BasicBlock-66            [-1, 512, 8, 8]               0\n",
      "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
      "           Linear-68                  [-1, 128]          65,664\n",
      "          Sigmoid-69                  [-1, 128]               0\n",
      "           Linear-70                    [-1, 4]             516\n",
      "================================================================\n",
      "Total params: 11,242,692\n",
      "Trainable params: 66,180\n",
      "Non-trainable params: 11,176,512\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.68\n",
      "Forward/backward pass size (MB): 76.08\n",
      "Params size (MB): 42.89\n",
      "Estimated Total Size (MB): 119.65\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(params_to_update, lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "summary(pre_model_Resnet,(3,244,244))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_epoch_C = []\n",
    "train_acc, test_acc = [], []\n",
    "best_acc, best_auc = 0.0, 0.0\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 / 50\n",
      "Training epoch: 1 / loss_C: 0.212 | acc: -0.034\n",
      "Testing acc: 0.295\n",
      "Cost 18.963(secs)\n",
      "epoch: 2 / 50\n",
      "Training epoch: 2 / loss_C: 0.160 | acc: 0.206\n",
      "Testing acc: 0.314\n",
      "Cost 19.594(secs)\n",
      "epoch: 3 / 50\n",
      "Training epoch: 3 / loss_C: 0.157 | acc: 0.214\n",
      "Testing acc: 0.274\n",
      "Cost 20.264(secs)\n",
      "epoch: 4 / 50\n",
      "Training epoch: 4 / loss_C: 0.155 | acc: 0.215\n",
      "Testing acc: 0.302\n",
      "Cost 20.222(secs)\n",
      "epoch: 5 / 50\n",
      "Training epoch: 5 / loss_C: 0.155 | acc: 0.219\n",
      "Testing acc: 0.279\n",
      "Cost 20.552(secs)\n",
      "epoch: 6 / 50\n",
      "Training epoch: 6 / loss_C: 0.158 | acc: 0.214\n",
      "Testing acc: 0.257\n",
      "Cost 20.477(secs)\n",
      "epoch: 7 / 50\n",
      "Training epoch: 7 / loss_C: 0.155 | acc: 0.220\n",
      "Testing acc: 0.297\n",
      "Cost 20.437(secs)\n",
      "epoch: 8 / 50\n",
      "Training epoch: 8 / loss_C: 0.153 | acc: 0.221\n",
      "Testing acc: 0.281\n",
      "Cost 20.637(secs)\n",
      "epoch: 9 / 50\n",
      "Training epoch: 9 / loss_C: 0.153 | acc: 0.225\n",
      "Testing acc: 0.293\n",
      "Cost 20.953(secs)\n",
      "epoch: 10 / 50\n",
      "Training epoch: 10 / loss_C: 0.153 | acc: 0.224\n",
      "Testing acc: 0.314\n",
      "Cost 20.908(secs)\n",
      "epoch: 11 / 50\n",
      "Training epoch: 11 / loss_C: 0.153 | acc: 0.230\n",
      "Testing acc: 0.306\n",
      "Cost 22.203(secs)\n",
      "epoch: 12 / 50\n",
      "Training epoch: 12 / loss_C: 0.153 | acc: 0.230\n",
      "Testing acc: 0.292\n",
      "Cost 22.074(secs)\n",
      "epoch: 13 / 50\n",
      "Training epoch: 13 / loss_C: 0.153 | acc: 0.227\n",
      "Testing acc: 0.288\n",
      "Cost 21.049(secs)\n",
      "epoch: 14 / 50\n",
      "Training epoch: 14 / loss_C: 0.152 | acc: 0.226\n",
      "Testing acc: 0.300\n",
      "Cost 20.334(secs)\n",
      "epoch: 15 / 50\n",
      "Training epoch: 15 / loss_C: 0.153 | acc: 0.229\n",
      "Testing acc: 0.286\n",
      "Cost 20.336(secs)\n",
      "epoch: 16 / 50\n",
      "Training epoch: 16 / loss_C: 0.151 | acc: 0.233\n",
      "Testing acc: 0.299\n",
      "Cost 20.771(secs)\n",
      "epoch: 17 / 50\n",
      "Training epoch: 17 / loss_C: 0.151 | acc: 0.232\n",
      "Testing acc: 0.309\n",
      "Cost 20.853(secs)\n",
      "epoch: 18 / 50\n",
      "Training epoch: 18 / loss_C: 0.151 | acc: 0.230\n",
      "Testing acc: 0.281\n",
      "Cost 20.400(secs)\n",
      "epoch: 19 / 50\n",
      "Training epoch: 19 / loss_C: 0.151 | acc: 0.234\n",
      "Testing acc: 0.307\n",
      "Cost 20.694(secs)\n",
      "epoch: 20 / 50\n",
      "Training epoch: 20 / loss_C: 0.150 | acc: 0.237\n",
      "Testing acc: 0.288\n",
      "Cost 20.466(secs)\n",
      "epoch: 21 / 50\n",
      "Training epoch: 21 / loss_C: 0.151 | acc: 0.240\n",
      "Testing acc: 0.295\n",
      "Cost 20.423(secs)\n",
      "epoch: 22 / 50\n",
      "Training epoch: 22 / loss_C: 0.150 | acc: 0.233\n",
      "Testing acc: 0.277\n",
      "Cost 20.717(secs)\n",
      "epoch: 23 / 50\n",
      "Training epoch: 23 / loss_C: 0.150 | acc: 0.238\n",
      "Testing acc: 0.285\n",
      "Cost 20.576(secs)\n",
      "epoch: 24 / 50\n",
      "Training epoch: 24 / loss_C: 0.149 | acc: 0.235\n",
      "Testing acc: 0.291\n",
      "Cost 21.448(secs)\n",
      "epoch: 25 / 50\n",
      "Training epoch: 25 / loss_C: 0.148 | acc: 0.246\n",
      "Testing acc: 0.271\n",
      "Cost 21.810(secs)\n",
      "epoch: 26 / 50\n",
      "Training epoch: 26 / loss_C: 0.148 | acc: 0.243\n",
      "Testing acc: 0.267\n",
      "Cost 20.936(secs)\n",
      "epoch: 27 / 50\n",
      "Training epoch: 27 / loss_C: 0.149 | acc: 0.237\n",
      "Testing acc: 0.259\n",
      "Cost 21.954(secs)\n",
      "epoch: 28 / 50\n",
      "Training epoch: 28 / loss_C: 0.148 | acc: 0.241\n",
      "Testing acc: 0.270\n",
      "Cost 20.883(secs)\n",
      "epoch: 29 / 50\n",
      "Training epoch: 29 / loss_C: 0.148 | acc: 0.244\n",
      "Testing acc: 0.294\n",
      "Cost 20.303(secs)\n",
      "epoch: 30 / 50\n",
      "Training epoch: 30 / loss_C: 0.148 | acc: 0.240\n",
      "Testing acc: 0.290\n",
      "Cost 20.452(secs)\n",
      "epoch: 31 / 50\n",
      "Training epoch: 31 / loss_C: 0.148 | acc: 0.248\n",
      "Testing acc: 0.290\n",
      "Cost 21.034(secs)\n",
      "epoch: 32 / 50\n",
      "Training epoch: 32 / loss_C: 0.148 | acc: 0.240\n",
      "Testing acc: 0.291\n",
      "Cost 20.649(secs)\n",
      "epoch: 33 / 50\n",
      "Training epoch: 33 / loss_C: 0.152 | acc: 0.238\n",
      "Testing acc: 0.274\n",
      "Cost 21.595(secs)\n",
      "epoch: 34 / 50\n",
      "Training epoch: 34 / loss_C: 0.148 | acc: 0.242\n",
      "Testing acc: 0.295\n",
      "Cost 20.753(secs)\n",
      "epoch: 35 / 50\n",
      "Training epoch: 35 / loss_C: 0.147 | acc: 0.246\n",
      "Testing acc: 0.308\n",
      "Cost 20.924(secs)\n",
      "epoch: 36 / 50\n",
      "Training epoch: 36 / loss_C: 0.145 | acc: 0.249\n",
      "Testing acc: 0.277\n",
      "Cost 20.804(secs)\n",
      "epoch: 37 / 50\n",
      "Training epoch: 37 / loss_C: 0.145 | acc: 0.254\n",
      "Testing acc: 0.257\n",
      "Cost 20.287(secs)\n",
      "epoch: 38 / 50\n",
      "Training epoch: 38 / loss_C: 0.145 | acc: 0.251\n",
      "Testing acc: 0.268\n",
      "Cost 20.072(secs)\n",
      "epoch: 39 / 50\n",
      "Training epoch: 39 / loss_C: 0.145 | acc: 0.251\n",
      "Testing acc: 0.249\n",
      "Cost 20.281(secs)\n",
      "epoch: 40 / 50\n",
      "Training epoch: 40 / loss_C: 0.150 | acc: 0.257\n",
      "Testing acc: 0.276\n",
      "Cost 20.148(secs)\n",
      "epoch: 41 / 50\n",
      "Training epoch: 41 / loss_C: 0.148 | acc: 0.255\n",
      "Testing acc: 0.232\n",
      "Cost 20.204(secs)\n",
      "epoch: 42 / 50\n",
      "Training epoch: 42 / loss_C: 0.144 | acc: 0.247\n",
      "Testing acc: 0.267\n",
      "Cost 20.438(secs)\n",
      "epoch: 43 / 50\n",
      "Training epoch: 43 / loss_C: 0.146 | acc: 0.250\n",
      "Testing acc: 0.263\n",
      "Cost 21.352(secs)\n",
      "epoch: 44 / 50\n",
      "Training epoch: 44 / loss_C: 0.149 | acc: 0.243\n",
      "Testing acc: 0.264\n",
      "Cost 20.942(secs)\n",
      "epoch: 45 / 50\n",
      "Training epoch: 45 / loss_C: 0.144 | acc: 0.256\n",
      "Testing acc: 0.244\n",
      "Cost 20.436(secs)\n",
      "epoch: 46 / 50\n",
      "Training epoch: 46 / loss_C: 0.145 | acc: 0.249\n",
      "Testing acc: 0.261\n",
      "Cost 20.915(secs)\n",
      "epoch: 47 / 50\n",
      "Training epoch: 47 / loss_C: 0.143 | acc: 0.257\n",
      "Testing acc: 0.261\n",
      "Cost 20.264(secs)\n",
      "epoch: 48 / 50\n",
      "Training epoch: 48 / loss_C: 0.142 | acc: 0.263\n",
      "Testing acc: 0.252\n",
      "Cost 20.085(secs)\n",
      "epoch: 49 / 50\n",
      "Training epoch: 49 / loss_C: 0.145 | acc: 0.251\n",
      "Testing acc: 0.254\n",
      "Cost 22.050(secs)\n",
      "epoch: 50 / 50\n",
      "Training epoch: 50 / loss_C: 0.144 | acc: 0.254\n",
      "Testing acc: 0.253\n",
      "Cost 25.528(secs)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "        start_time = time.time()\n",
    "        iter = 0\n",
    "        correct_train, total_train = 0, 0\n",
    "        correct_test, total_test = 0, 0\n",
    "        train_loss_C = 0.0\n",
    "        total_mae = 0.0\n",
    "\n",
    "        pre_model_Resnet.train() # 設定 train 或 eval\n",
    "        print('epoch: ' + str(epoch + 1) + ' / ' + str(epochs))  \n",
    "        \n",
    "        # ---------------------------\n",
    "        # Training Stage\n",
    "        # ---------------------------\n",
    "        for i, d in enumerate(train_dataLoader) :\n",
    "            x, label = d['image'].to(device), d['psi'].to(device).float().squeeze(dim=1)\n",
    "            # print(label.shape)\n",
    "            optimizer.zero_grad()                           # 清空梯度\n",
    "            train_output = pre_model_Resnet(x)              # 將訓練資料輸入至模型進行訓練 (Forward propagation)\n",
    "            # print(label.size)\n",
    "            train_loss = criterion(train_output, label)     # 計算 loss\n",
    "            train_loss.backward()                           # 將 loss 反向傳播\n",
    "            optimizer.step()                                # 更新權重\n",
    "\n",
    "            train_loss_C += train_loss.item()\n",
    "            mae = (torch.abs(train_output - label).sum()/4).item()\n",
    "            total_mae += mae    \n",
    "            \n",
    "            # 計算訓練資料的準確度 (correct_train / total_train)\n",
    "            # _, predicted = torch.max(train_output.data, 1)  # 取出預測的 maximum\n",
    "            # total_train += label.size(0)\n",
    "            # correct_train += (predicted == label).sum()\n",
    "            # train_loss_C += train_loss.item()\n",
    "            iter += 1\n",
    "                    \n",
    "        print('Training epoch: %d / loss_C: %.3f | acc: %.3f' % (epoch + 1, train_loss_C / iter, 1-total_mae / iter))\n",
    "        \n",
    "        # --------------------------\n",
    "        # Testing Stage\n",
    "        # --------------------------\n",
    "        pre_model_Resnet.eval() # 設定 train 或 eval\n",
    "        iter = 0\n",
    "        total_mae = 0.0\n",
    "        for i, d in enumerate(test_dataLoader) :\n",
    "            with torch.no_grad():                           # 測試階段不需要求梯度\n",
    "                x, label = d['image'].to(device), d['psi'].to(device).float().squeeze(dim=1)\n",
    "                test_output = pre_model_Resnet(x)                          # 將測試資料輸入至模型進行測試\n",
    "                test_loss = criterion(test_output, label)   # 計算 loss\n",
    "                \n",
    "                mae = (torch.abs(test_output - label).sum()/4).item()\n",
    "                total_mae += mae\n",
    "                iter+=1\n",
    "                # 計算測試資料的準確度 (correct_test / total_test)\n",
    "                # _, predicted = torch.max(test_output.data, 1)\n",
    "                # total_test += label.size(0)\n",
    "                # correct_test += (predicted == label).sum()\n",
    "        \n",
    "        print('Testing acc: %.3f' % (1-total_mae / iter))\n",
    "                                     \n",
    "        # train_acc.append(100 * (correct_train / total_train).cpu()) # training accuracy\n",
    "        # test_acc.append(100 * (correct_test / total_test).cpu())    # testing accuracy\n",
    "        # loss_epoch_C.append((train_loss_C / iter))            # loss \n",
    "\n",
    "        end_time = time.time()\n",
    "        print('Cost %.3f(secs)' % (end_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
